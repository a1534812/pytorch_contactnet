import os
import os.path
import sys
import numpy as np
import math
import random

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, utils
from torch.utils.data import DataLoader, Dataset
from keras.utils.np_utils import to_categorical
# TO-DO: put import of dataset here

from pointnet2.models_pointnet import PointNet, SAModule

class ContactNet(nn.Module):
    def __init__(self, generate, latent_size, device):
        super().__init__()
        self.device = device
        if generate == 'vae':
            self.sampler = GraspVAE(latent_size, device)
        elif generate == 'ilme':
            self.sampler = GraspILME(latent_size, device)

    def forward(self, inputpcd):

        # sample points on the pointcloud to generate grasps from

        # pass entire pointcloud through PointNet to obtain point features

        # get pointwise features for sampled points

        # sample grasps with pt features as conditions

        # 

class GraspSampler(nn.Module):
    def __init__(self, latent_size, device):
        super(GraspSampler, self).__init__()
        self.latent_size = latent_size
        self.device = device

    def create_decoder(self, model_scale, pointnet_radius, pointnet_nclusters,
                       num_input_features):
        # The number of input features for the decoder is 3+latent space where 3
        # represents the x, y, z position of the point-cloud

        self.decoder = base_network(pointnet_radius, pointnet_nclusters,
                                    model_scale, num_input_features)
        self.q = nn.Linear(model_scale * 1024, 4)
        self.t = nn.Linear(model_scale * 1024, 3)
        self.confidence = nn.Linear(model_scale * 1024, 1)

    def decode(self, xyz, z):
        xyz_features = self.concatenate_z_with_pc(xyz, z).transpose(-1, 1).contiguous()
        for module in self.decoder[0]:
            xyz, xyz_features = module(xyz, xyz_features)
        x = self.decoder[1](xyz_features.squeeze(-1))
        predicted_qt = torch.cat((F.normalize(self.q(x), p=2, dim=-1), self.t(x)), -1)

        return predicted_qt, torch.sigmoid(self.confidence(x)).squeeze()

    def concatenate_z_with_pc(self, pc, z):
        z.unsqueeze_(1)
        z = z.expand(-1, pc.shape[1], -1)
        return torch.cat((pc, z), -1)

    def get_latent_size(self):
        return self.latent_size

class GraspVAE(GraspSampler):
    """Network for learning a generative VAE grasp-sampler
    """
    def __init__(self,
                 model_scale,
                 pointnet_radius=0.02,
                 pointnet_nclusters=128,
                 latent_size=2,
                 device="cpu"):
        super(GraspVAE, self).__init__(latent_size, device)
        self.create_encoder(model_scale, pointnet_radius, pointnet_nclusters)

        self.create_decoder(model_scale, pointnet_radius, pointnet_nclusters,
                            latent_size + 3)
        self.create_bottleneck(model_scale * 1024, latent_size)

    def create_encoder(self, model_scale, pointnet_radius, pointnet_nclusters):
        # The number of input features for the encoder is 19: the x, y, z
        # position of the point-cloud and the flattened 4x4=16 grasp pose matrix
        self.encoder = base_network(pointnet_radius, pointnet_nclusters,
                                    model_scale, 19)

    def create_bottleneck(self, input_size, latent_size):
        mu = nn.Linear(input_size, latent_size)
        logvar = nn.Linear(input_size, latent_size)
        self.latent_space = nn.ModuleList([mu, logvar])

    def encode(self, xyz, xyz_features):
        for module in self.encoder[0]:
            xyz, xyz_features = module(xyz, xyz_features)
        return self.encoder[1](xyz_features.squeeze(-1))

    def bottleneck(self, z):
        return self.latent_space[0](z), self.latent_space[1](z)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, pc, grasp=None, train=True):
        if train:
            return self.forward_train(pc, grasp)
        else:
            return self.forward_test(pc, grasp)

    def forward_train(self, pc, grasp):
        input_features = torch.cat(
            (pc, grasp.unsqueeze(1).expand(-1, pc.shape[1], -1)),
            -1).transpose(-1, 1).contiguous()
        z = self.encode(pc, input_features)
        mu, logvar = self.bottleneck(z)
        z = self.reparameterize(mu, logvar)
        qt, confidence = self.decode(pc, z)
        return qt, confidence, mu, logvar

    def forward_test(self, pc, grasp):
        input_features = torch.cat(
            (pc, grasp.unsqueeze(1).expand(-1, pc.shape[1], -1)),
            -1).transpose(-1, 1).contiguous()
        z = self.encode(pc, input_features)
        mu, _ = self.bottleneck(z)
        qt, confidence = self.decode(pc, mu)
        return qt, confidence

    def sample_latent(self, batch_size):
        return torch.randn(batch_size, self.latent_size).to(self.device)

    def generate_grasps(self, pc, z=None):
        if z is None:
            z = self.sample_latent(pc.shape[0])
        qt, confidence = self.decode(pc, z)
        return qt, confidence, z.squeeze()

    def generate_dense_latents(self, resolution):
        """
        For the VAE sampler we consider dense latents to correspond to those between -2 and 2
        """
        latents = torch.meshgrid(*[
            torch.linspace(-2, 2, resolution) for i in range(self.latent_size)
        ])
        return torch.stack([latents[i].flatten() for i in range(len(latents))],
                           dim=-1).to(self.device)


class GraspILME(GraspSampler):
    """
    Altough the name says this sampler is based on the GAN formulation, it is
    not actually optimizing based on the commonly known adversarial game.
    Instead, it is based on the Implicit Maximum Likelihood Estimation from
    https://arxiv.org/pdf/1809.09087.pdf which is similar to the GAN formulation
    but with new insights that avoids e.g. mode collapses.
    """
    def __init__(self,
                 model_scale,
                 pointnet_radius,
                 pointnet_nclusters,
                 latent_size=2,
                 device="cpu"):
        super(GraspILME, self).__init__(latent_size, device)
        self.create_decoder(model_scale, pointnet_radius, pointnet_nclusters,
                            latent_size + 3)

    def sample_latent(self, batch_size):
        return torch.rand(batch_size, self.latent_size).to(self.device)

    def forward(self, pc, grasps=None, train=True):
        z = self.sample_latent(pc.shape[0])
        return self.decode(pc, z)

    def generate_grasps(self, pc, z=None):
        if z is None:
            z = self.sample_latent(pc.shape[0])
        qt, confidence = self.decode(pc, z)
        return qt, confidence, z.squeeze()

    def generate_dense_latents(self, resolution):
        latents = torch.meshgrid(*[
            torch.linspace(0, 1, resolution) for i in range(self.latent_size)
        ])
        return torch.stack([latents[i].flatten() for i in range(len(latents))], dim=-1).to(self.device)

def base_network(pointnet_radius, pointnet_nclusters, scale, in_features):
    sa1_module = SAModule(
        npoint=pointnet_nclusters,
        radius=pointnet_radius,
        nsample=64,
        mlp=[in_features, 64 * scale, 64 * scale, 128 * scale])
    sa2_module = SAModule(
        npoint=32,
        radius=0.04,
        nsample=128,
        mlp=[128 * scale, 128 * scale, 128 * scale, 256 * scale])

    sa3_module = SAModule(
        mlp=[256 * scale, 256 * scale, 256 * scale, 512 * scale])

    sa_modules = nn.ModuleList([sa1_module, sa2_module, sa3_module])
    fc_layer = nn.Sequential(nn.Linear(512 * scale, 1024 * scale),
                             nn.BatchNorm1d(1024 * scale), nn.ReLU(True),
                             nn.Linear(1024 * scale, 1024 * scale),
                             nn.BatchNorm1d(1024 * scale), nn.ReLU(True))
    return nn.ModuleList([sa_modules, fc_layer])
